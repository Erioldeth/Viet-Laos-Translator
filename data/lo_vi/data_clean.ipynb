{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each line of the file into a list\n",
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    sentences = file.readlines()\n",
    "    file.close()\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# load text data from file\n",
    "file_name = 'clean_train2023'\n",
    "vi_data_path = file_name + '.vi'\n",
    "lo_data_path = file_name + '.lo'\n",
    "vi_data = load_doc(vi_data_path)\n",
    "lo_data = load_doc(lo_data_path)\n",
    "print('Loaded %s lines of data' % len(vi_data))\n",
    "print('Loaded %s lines of data' % len(lo_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "non_vn = []\n",
    "#go through vi_data and find sentences not detected as vietnamese\n",
    "\n",
    "for i in range(len(vi_data)):\n",
    "#handle no feature exception\n",
    "    try:\n",
    "        if detect(vi_data[i]) != 'vi':\n",
    "            non_vn.append(i)\n",
    "    except:\n",
    "        non_vn.append(i)\n",
    "\n",
    "#write non_vn to file\n",
    "non_vn_file = open('non_vn.txt', 'w')\n",
    "for i in non_vn:\n",
    "    #write data with (index i to file\n",
    "    non_vn_file.write(vi_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non_vn from vi_data and lo_data\n",
    "for i in sorted(non_vn, reverse=True):\n",
    "    del vi_data[i]\n",
    "    del lo_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print longest tokenized sentence in each dataset and its length and coreesponding sentence in other language\n",
    "max_len = 0\n",
    "max_len_index = 0\n",
    "for i in range(len(vi_data)):\n",
    "    if len(vi_data[i].split()) > max_len:\n",
    "        max_len = len(vi_data[i].split())\n",
    "        max_len_index = i\n",
    "print('Longest sentence in Vietnamese dataset is: ', vi_data[max_len_index])\n",
    "print('Length of longest sentence in Vietnamese dataset is: ', max_len)\n",
    "print('Sentence in Lao dataset is: ', lo_data[max_len_index])\n",
    "\n",
    "max_len = 0\n",
    "max_len_index = 0\n",
    "for i in range(len(lo_data)):\n",
    "    if len(lo_data[i].split()) > max_len:\n",
    "        max_len = len(lo_data[i].split())\n",
    "        max_len_index = i\n",
    "print('Longest sentence in Lao dataset is: ', lo_data[max_len_index])\n",
    "print('Length of longest sentence in Lao dataset is: ', max_len)\n",
    "print('Sentence in Vietnamese dataset is: ', vi_data[max_len_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicate lines and remove them in both files , keep the order\n",
    "def remove_duplicate(vi_data,lo_data):\n",
    "    vi_data_clean = []\n",
    "    lo_data_clean = []\n",
    "    for i in range(len(vi_data)):\n",
    "        if vi_data[i] not in vi_data_clean:\n",
    "            vi_data_clean.append(vi_data[i])\n",
    "            lo_data_clean.append(lo_data[i])\n",
    "    return vi_data_clean,lo_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_data,lo_data = remove_duplicate(vi_data,lo_data)\n",
    "print(len(vi_data))\n",
    "print(len(lo_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through Laos data and remove the lines that do not have any Laos characters, and remove the corresponding lines in Vietnamese data\n",
    "def remove_non_Laos(vi_data,lo_data):\n",
    "    vi_data_clean = []\n",
    "    lo_data_clean = []\n",
    "    lo_char = 'ກຂຄງຈຊຍດຕຖທນບປຜຝພຟມຢຣລວຫອຮຯະັາຳິີຶືຸູົຼຽເແໂໃໄໆ່້໊໋ໜໝໞໟ໠໡໢໣໤໥໦໧໨໩໪໫໬໭໮'\n",
    "    for i in range(len(lo_data)):\n",
    "        if any(char in lo_char for char in lo_data[i]):\n",
    "            vi_data_clean.append(vi_data[i])\n",
    "            lo_data_clean.append(lo_data[i])\n",
    "    return vi_data_clean,lo_data_clean\n",
    "\n",
    "vi_data,lo_data = remove_non_Laos(vi_data,lo_data)\n",
    "\n",
    "print(len(vi_data))\n",
    "print(len(lo_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwriting the files with the cleaned data\n",
    "def save_data(vi_data,lo_data):\n",
    "    clean_file_name = 'clean_' + file_name\n",
    "    vi_data_path = clean_file_name + '.vi'\n",
    "    lo_data_path = clean_file_name + '.lo'\n",
    "    file = open(vi_data_path, mode='wt', encoding='utf-8')\n",
    "    file.writelines(vi_data)\n",
    "    file.close()\n",
    "    file = open(lo_data_path, mode='wt', encoding='utf-8')\n",
    "    file.writelines(lo_data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "save_data(vi_data,lo_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
