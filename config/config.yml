# data location and config section
data:
  train_data_location: data/lo_vi/train2023
  valid_data_location: data/lo_vi/dev2013
  src_lang: .lo
  trg_lang: .vi
log_file_models: 'model.log'
build_vocab_kwargs: # additional arguments for build_vocab. See torchtext.vocab.Vocab for mode details
  min_freq: 5
# model parameters section
d_model: 512
n_layers: 6
heads: 8
# training config, evaluation, save & load section
train_max_length: 50 # training samples with this much length in src/trg will be discarded
train_batch_size: 64
epochs: 20
# inference section
valid_batch_size: 8
decode_strategy: BeamSearch
decode_strategy_kwargs:
  beam_size: 5 # beam search size
  length_normalize: 0.6 # recalculate beam position by length. Currently only work in default BeamSearch
  replace_unk: # tuple of layer/head attention to replace unknown words
    - 0 # layer
    - 0 # head
input_max_length: 200 # input longer than this value will be trimmed in inference. Note that this values are to be used during cached PE, hence, validation set with more than this many tokens will call a warning for the trimming.
max_length: 160 # only perform up to this much timestep during inference
# optimizer and learning arguments section
lr: !!float 1e-3
optimizer_params:
  betas:
    - 0.9 # beta1
    - 0.98 # beta2
  eps: !!float 1e-9
n_warmup_steps: 4000
label_smoothing: 0.1
dropout: 0.1
