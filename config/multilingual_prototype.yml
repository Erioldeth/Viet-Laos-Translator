# data section
data:
  train:
    zh_vi_ALT:
      path:  data/ALT/zh-vi/train.bpe
      src_ext: .zh
      trg_ext: .vi
      src_sos: <zh2vi>
      trg_sos: <sos>
   
    lo_vi_ALT:
      path: data/ALT/lo-vi/train.bpe
      src_ext: .lo
      trg_ext: .vi
      src_sos: <lo2vi>
      trg_sos: <sos>

    km_vi_ALT:
      path: data/ALT/km-vi/train.bpe
      src_ext: .km
      trg_ext: .vi
      src_sos: <km2vi>
      trg_sos: <sos>

    en_vi_iwslt:
      path: data/ALT/en-vi/train.bpe
      src_ext: .en
      trg_ext: .vi
      src_sos: <en2vi>
      trg_sos: <sos>
    
  valid:
    path: data/test/test.bpe
    src_ext: .zh
    trg_ext: .vi
    src_sos: <zh2vi>
    trg_sos: <sos>
log_file_models: 'model.log'   
lowercase: false
build_vocab_kwargs: # additional arguments for build_vocab. See torchtext.vocab.Vocab for mode details
#  max_size: 50000
  min_freq: 5
# model parameters section
device: cuda
d_model: 512
d_ff: 2048
n_encoder_layers: 12
n_decoder_layers: 6
heads: 8
norm: FusedNorm
# inference section
eval_batch_size: 8
decode_strategy: BeamSearch
decode_strategy_kwargs:
  beam_size: 4 # beam search size
  replace_unk: # tuple of layer/head attention to replace unknown words
    - 0 # layer
    - 0 # head
input_max_length: 200 # input longer than this value will be trimmed
max_length: 160 # only perform up to this much timestep during inference
train_max_length: 50 # training samples with this much length in src/trg will be discarded
# optimizer and learning arguments section
lr: 0.2
optimizer: Adam
optimizer_params:
  betas:
    - 0.9 # beta1
    - 0.98 # beta2
  eps: !!float 1e-9
n_warmup_steps: 4000
label_smoothing: 0.1
dropout: 0.1
# training config, evaluation, save & load section
batch_size: 64
epochs: 20
printevery: 200
save_checkpoint_epochs: 1
maximum_saved_model_eval: 5
maximum_saved_model_train: 5
